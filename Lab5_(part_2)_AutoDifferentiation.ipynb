{"cells":[{"cell_type":"markdown","source":["# Lab 5 (part 2): Auto-Differentiation"],"metadata":{"id":"A90Yoa3_1Aei"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NvS2sNRoJfPb"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","source":["# Toy problem\n","Suppose we want to fit a third order polynomial to predict $y=\\sin(x)$ in the interval $-\\pi \\le x \\le \\pi$.\n","\n","Let's start out by generating some training data $\\{y_i,x_i\\}_{i=1}^{N}$, where $i$ iterates over the $N$ training samples."],"metadata":{"id":"D2HZvxC6LEhe"}},{"cell_type":"code","source":["import torch\n","import math\n","from matplotlib import pyplot as plt\n","import numpy as np\n","\n","dtype = torch.float\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","torch.set_default_device(device)\n","\n","# Create Tensors to hold input and outputs.\n","# By default, requires_grad=False, which indicates that we do not need to\n","# compute gradients with respect to these Tensors during the backward pass.\n","N = 20\n","x = torch.linspace(-math.pi, math.pi, N, dtype=dtype)\n","y = torch.sin(x)\n","\n","plt.plot(x.numpy(),y.numpy(),'.')"],"metadata":{"id":"ps_AQwOmKFal"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Manual derivatives\n","Now, let's fit the training data using a third order polynomial by minimizing the L2 norm.\n","\n","Writing the i'th prediction as $y_{pred,i}=a+bx_i+cx_i^2+dx_i^3$, we would like our model to output $y_{pred,i} \\approx y_i$.\n","\n","Manually calculating the partial derivatives of the loss function w.r.t. $a$, $b$, $c$, and $d$ would not a big deal for this toy example. We just need to differentitate the loss function\n","\n","$J = \\sum{(y_{pred,i}-y_i)^2} = \\sum{(a+bx_i+cx_i^2+dx_i^3 - y_i)^2}$\n","\n","where we are summing over the training samples.\n","\n","Applying the chain rule a few times, we can easily show that\n","\n","$\\frac{\\partial J}{\\partial a} = 2\\sum{y_{pred,i}-y_i}$\n","\n","$\\frac{\\partial J}{\\partial b} = 2\\sum{x_i(y_{pred,i}-y_i)}$\n","\n","$\\frac{\\partial J}{\\partial c} = 2\\sum{x_i^2(y_{pred,i}-y_i)}$\n","\n","$\\frac{\\partial J}{\\partial d} = 2\\sum{x_i^3(y_{pred,i}-y_i)}$\n","\n","We now have what we need to fit the polynomial using gradient descent.\n","\n","**Your task:** Fill in the gaps below (marked with `???`) and run the code."],"metadata":{"id":"BcfoOXyMV0aZ"}},{"cell_type":"code","source":["# Create random Tensors for weights.\n","a = torch.randn((), device=device, dtype=dtype)\n","b = torch.randn((), device=device, dtype=dtype)\n","c = torch.randn((), device=device, dtype=dtype)\n","d = torch.randn((), device=device, dtype=dtype)\n","\n","learning_rate = 1e-4\n","for t in range(2000):\n","    # Forward pass: compute predicted y\n","    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n","\n","    # Compute and print loss\n","    loss = (y_pred - y).pow(2).sum().item()\n","    if t % 100 == 99:\n","        print(t, loss)\n","\n","    # Compute gradients of loss w.r.t. a, b, c, and d\n","    grad_y_pred = 2.0 * (y_pred - y)\n","    grad_a = grad_y_pred.sum()\n","    grad_b = (grad_y_pred * x).sum()\n","    grad_c = (grad_y_pred * x ** 2).sum()\n","    grad_d = ???\n","\n","    # Update weights using gradient descent\n","    a = a - learning_rate * grad_a\n","    b = b - learning_rate * grad_b\n","    c = c - learning_rate * grad_c\n","    d = d - ???\n","\n","print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n","plt.plot(x.numpy(),y.numpy(),'.',x.numpy(),y_pred.detach().numpy())"],"metadata":{"id":"D1FYbgHaVhnH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Automatic differentiation\n","Manual differentiation can quickly get very hairy for large complex networks. Thankfully, we can use automatic differentiation to automate the computation of backward passes in our model (or neural network). The autograd package in PyTorch provides exactly this functionality. When using autograd, the forward pass of your model will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients.\n","\n","This sounds complicated, it’s pretty simple to use in practice. Each Tensor represents a node in a computational graph. If x is a Tensor that has x.requires_grad=True then x.grad is another Tensor holding the gradient of x with respect to some scalar value.\n","\n","Here we use PyTorch Tensors and autograd to implement our fitting sine wave with third order polynomial example; now we don't need to manually implement the backward pass through the model.\n","\n","**Your task:** Uncomment the correct `backward()` call and fill in the gaps marked with `???`."],"metadata":{"id":"bwA2prZhMRJI"}},{"cell_type":"code","source":["# Create random Tensors for weights.\n","# Setting requires_grad=True indicates that we want to compute gradients with\n","# respect to these Tensors during the backward pass.\n","a = torch.randn((), dtype=dtype, requires_grad=True)\n","b = torch.randn((), dtype=dtype, requires_grad=True)\n","c = torch.randn((), dtype=dtype, requires_grad=True)\n","d = torch.randn((), dtype=dtype, requires_grad=True)\n","\n","learning_rate = 1e-4\n","for t in range(2000):\n","    # Forward pass: compute predicted y using operations on Tensors.\n","    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n","\n","    # Compute and print loss using operations on Tensors.\n","    # Now loss is a Tensor of shape (1,)\n","    # loss.item() gets the scalar value held in the loss.\n","    loss = (y_pred - y).pow(2).sum()\n","    if t % 100 == 99:\n","        print(t, loss.item())\n","\n","    # Use autograd to compute the backward pass. This call will compute the\n","    # gradient of the loss with respect to all Tensors with requires_grad=True.\n","    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n","    # the gradient of the loss with respect to a, b, c, d respectively.\n","\n","    # TASK: Uncomment the correct backward() call\n","    #y_pred.backward()\n","    #loss.backward()\n","    #x.backward()\n","\n","    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n","    # because weights have requires_grad=True, but we don't need to track this\n","    # in autograd.\n","    with torch.no_grad():\n","        a -= learning_rate * a.grad\n","        b -= learning_rate * b.grad\n","        c -= learning_rate * c.grad\n","        d -= ???\n","\n","        # Manually zero the gradients after updating weights\n","        a.grad = None\n","        b.grad = None\n","        c.grad = None\n","        d.grad = ???\n","\n","print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n","plt.plot(x.numpy(),y.numpy(),'.',x.numpy(),y_pred.detach().numpy())"],"metadata":{"id":"vLQCdxDpJjMl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eO1YlSMUJfPf"},"source":["# Defining New autograd Functions\n","Instead of writing the polynomial as $y=a+bx+cx^2+dx^3$, we might want to write it as $y=a+b P_3(c+dx)$ where $P_3(x)=\\frac{1}{2}\\left(5x^3-3x\\right)$ is\n","the [*Legendre polynomial*](https://en.wikipedia.org/wiki/Legendre_polynomials) of degree three.\n","\n","This implementation computes the forward pass using operations on PyTorch\n","Tensors, and uses PyTorch autograd to compute gradients.\n","\n","All we need to enable autograd is to implement a backward pass that calculates the *local gradient*, $P_3'(x)=\\frac{3}{2}\\left(5x^2-1\\right)$.\n","\n","**Your task:** Fill in the gap marked with `???`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"linlrMOLJfPg"},"outputs":[],"source":["import torch\n","import math\n","\n","class LegendrePolynomial3(torch.autograd.Function):\n","    \"\"\"\n","    We can implement our own custom autograd Functions by subclassing\n","    torch.autograd.Function and implementing the forward and backward passes\n","    which operate on Tensors.\n","    \"\"\"\n","\n","    @staticmethod\n","    def forward(ctx, input):\n","        \"\"\"\n","        In the forward pass we receive a Tensor containing the input and return\n","        a Tensor containing the output. ctx is a context object that can be used\n","        to stash information for backward computation. You can cache arbitrary\n","        objects for use in the backward pass using the ctx.save_for_backward method.\n","        \"\"\"\n","        ctx.save_for_backward(input)\n","        return 0.5 * (5 * input ** 3 - 3 * input)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        \"\"\"\n","        In the backward pass we receive a Tensor containing the gradient of the loss\n","        with respect to the output, and we need to compute the gradient of the loss\n","        with respect to the input.\n","        \"\"\"\n","        input, = ctx.saved_tensors\n","        return grad_output * 1.5 * (5 * input ** 2 - 1)\n","\n","# Create random Tensors for weights. For this example, we need\n","# 4 weights: y = a + b * P3(c + d * x), these weights need to be initialized\n","# not too far from the correct result to ensure convergence.\n","# Setting requires_grad=True indicates that we want to compute gradients with\n","# respect to these Tensors during the backward pass.\n","a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n","b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n","c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n","d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n","\n","learning_rate = 1e-3\n","for t in range(2000):\n","    # To apply our Function, we use Function.apply method. We alias this as 'P3'.\n","    P3 = LegendrePolynomial3.apply\n","\n","    # Forward pass: compute predicted y using operations; we compute\n","    # P3 using our custom autograd operation.\n","    y_pred = a + b * P3(???)\n","\n","    # Compute and print loss\n","    loss = (y_pred - y).pow(2).sum()\n","    if t % 100 == 99:\n","        print(t, loss.item())\n","\n","    # Use autograd to compute the backward pass.\n","    loss.backward()\n","\n","    # Update weights using gradient descent\n","    with torch.no_grad():\n","        a -= learning_rate * a.grad\n","        b -= learning_rate * b.grad\n","        c -= learning_rate * c.grad\n","        d -= learning_rate * d.grad\n","\n","        # Manually zero the gradients after updating weights\n","        a.grad = None\n","        b.grad = None\n","        c.grad = None\n","        d.grad = None\n","\n","print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')\n","plt.plot(x.numpy(),y.numpy(),'.',x.numpy(),y_pred.detach().numpy())"]},{"cell_type":"markdown","source":["# Chain rule in a neuron\n","**Question:** What's the point of defining a new autograd function, like we just did?\n","\n","**Background:**\n","\n","We can imagine a neural network as a massive computational graph. PyTorch automatically sets up that graph for us.\n","\n","Let us say we have a “neuron” f in that computational graph with inputs $x$ and $y$ which outputs $z$.\n","\n","We can easily compute the local gradients — differentiating $z$ with respect to $x$ and $y$ as $\\frac{\\partial z}{\\partial x}$ and $\\frac{\\partial z}{\\partial y}$.\n","\n","From the forward pass, we obtain the loss ($L$).\n","\n","When we start to work the loss backwards, we get the gradient of the loss from the layer above $\\frac{\\partial L}{\\partial z}$\n","\n","In order for the loss to be propagated backwards, we need to find $\\frac{\\partial L}{\\partial x}$ and $\\frac{\\partial L}{\\partial y}$.\n","\n","These can be calculated using the chain rule, i.e., $\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial x}$ and $\\frac{\\partial L}{\\partial y} = \\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial y}$\n","\n","![picture](https://miro.medium.com/v2/resize:fit:720/format:webp/1*NpYmr8RwZ3x32GTBUkbB4A.png)\n","\n","**Answer:**\n","\n","All we need to build computational graphs that enable automatic differentiation is to define a **forward** function and a **backward** function for each of the basic  operations (or neurons) we want in our model. We only need to do this once per operation, thereby avoiding the need for manual differentiation everytime we change the architecture of our model.\n","\n","The **forward** function calculates the output ($z$).\n","\n","The **backward** function calculates the local gradients (e.g., $\\frac{\\partial z}{\\partial x}$ and $\\frac{\\partial z}{\\partial y}$).\n","\n","All the rest is taken care of by applying the chain rule, as explained above.\n","\n","In the example from before, we defined those two functions for a *Legendre polynomial of degree three*. Having defined this neuron/operation once, we can use it as a building block to build models that can be automatically differentiated.\n","\n","PyTorch, of cause, comes with such an implementation for all the common building blocks, such as multiplication, addition, sigmoid, convolution, etc."],"metadata":{"id":"GQ5FA1_RdjO4"}},{"cell_type":"code","source":[],"metadata":{"id":"BrNlnjMzbei8"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/36007a4d548729b824a1364eb165d070/polynomial_custom_function.ipynb","timestamp":1694609651491}]}},"nbformat":4,"nbformat_minor":0}