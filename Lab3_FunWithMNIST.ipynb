{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1umMjAG8xg2TwTmGRY-BBCGRynKRh-SaK","timestamp":1693756839024},{"file_id":"1yAtTAxiaR18P2upYKXX05VMkoP2Zp86e","timestamp":1693753521687}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"REaV1GblZovs"},"source":["#Fun with MNIST\n","Convolutional Neural Networks (CNNs) are able to solve a wide range of computer vision tasks. In this Lab you will learn about\n","\n","- Image classification\n","- Convolutional AutoEncoders\n","- Denoising AutoEncoders\n","- Image super resolution\n","- Image regression\n","- Image segmentation\n","- Object detection\n","- Few-shot learning with Siamese networks\n","- Generative Adversarial Networks (GANs)\n","\n","The purpose of the Lab is to give your some intuition about how to tweak CNNs to solve different tasks.\n","\n","**Before we start - remember to set runtime to GPU**"]},{"cell_type":"markdown","metadata":{"id":"Qc8RJkIJaxap"},"source":["##Task 1: Downloading and pre-processing the MNIST dataset\n","The MNIST dataset of handwritten digits is so commonly used that it comes with most deep learning frameworks, including Keras. Let's download the dataset and explore a little bit."]},{"cell_type":"code","metadata":{"id":"r-_9ZwQkiGQg"},"source":["from __future__ import print_function\n","import tensorflow\n","from tensorflow import keras\n","from keras.datasets import mnist\n","from keras import backend as K\n","from matplotlib import pyplot as plt\n","\n","# Utility function for showing images\n","def show_imgs(x_test, n=10):\n","    sz = x_test.shape[1]\n","    plt.figure(figsize=(20, 4))\n","    for i in range(n):\n","        ax = plt.subplot(2, n, i+1)\n","        plt.imshow(x_test[i].reshape(sz,sz))\n","        plt.gray()\n","        ax.get_xaxis().set_visible(False)\n","        ax.get_yaxis().set_visible(False)\n","    plt.show()\n","\n","num_classes = 10\n","\n","# input image dimensions\n","img_rows, img_cols = 28, 28\n","\n","# the data, split between train and test sets\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","if K.image_data_format() == 'channels_first':\n","    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n","    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n","    input_shape = (1, img_rows, img_cols)\n","else:\n","    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n","    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n","    input_shape = (img_rows, img_cols, 1)\n","\n","# Pre-process inputs\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train /= 255\n","x_test /= 255\n","\n","# Convert class indices to one-hot vectors\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","print('Examples of test images')\n","show_imgs(x_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xHvyXrdliPFE"},"source":["###Questions 1.1\n","1. What is the input shape?\n","2. How many training examples are there?\n","3. How many test examples are there?\n","4. What does `to_categorical` do? (Hint: print `y_test` before and after applying `to_categorial`)."]},{"cell_type":"markdown","metadata":{"id":"5OX-mswCi2vY"},"source":["##Task 2: Softmax regression\n","Now, let's define a tiny Keras model for logistic regression. Mathematically this model outputs a 10-dimensional vector `y` of class probabilities, where\n","\n","\n","```\n","y = softmax(W*x + b)\n","```\n","\n","and\n","- `x` is a 28x28 = 784-dimensional vector corresponding to the input image,\n","- `W` is a 10 x 784 matrix of weights\n","- `b` is a 10-dimensional vector of biases\n","\n","Defining models in Keras is not very intuitive from a mathematical perspective. Here is one way to implement the equation above using Keras' [Sequential API](https://keras.io/getting-started/sequential-model-guide/). A Sequential model is a just a linear stack of layers."]},{"cell_type":"code","metadata":{"id":"HSqZ3O4ocxQ-"},"source":["from keras.models import Sequential\n","from keras.layers import Dense, Flatten, Activation\n","\n","tensorflow.random.set_seed(0) # make weight initialization deterministic\n","\n","# Model\n","model = Sequential()\n","model.add(Flatten(input_shape=input_shape)) # input_shape is (28, 28, 1)\n","model.add(Dense(num_classes)) # num_classes is 10\n","model.add(Activation('softmax'))\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CJBX3bW9kaq8"},"source":["###Questions 2.1\n","1. What do you think the Flatten() layer does, and why is it necessary to have it here?\n","2. What do you think the Dense() layer does, and why does it have 7850 parameters?\n","3. What do you think Activation() does, and why is its argument set to \"softmax\"?\n","\n","**Tip**\n","\n","To answer questions like the ones above, you could perform simple experiments like the one below:\n","\n","```\n","input_shape = (28, 28, 1)\n","x = tensorflow.random.normal(input_shape)\n","y = keras.layers.Flatten()(x)\n","print(y.shape)\n","```"]},{"cell_type":"markdown","metadata":{"id":"7PqpHs6EgXMP"},"source":["###Training"]},{"cell_type":"markdown","metadata":{"id":"eABHA5aSmBdy"},"source":["Now, let's train the model for 10 epochs. We will be using the multi-class version of the cross entropy loss and stochastic gradient descent (SGD). The difference between normal gradient descent and SGD is that normal gradient descent calculates the gradients based on all training examples, whereas SGD approximates the gradient by calculating it on small batches (of size 128 in this example)."]},{"cell_type":"code","metadata":{"id":"qimN5at5djhI"},"source":["batch_size = 128\n","epochs = 10\n","\n","# Compile the model before training\n","model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","# Fit model\n","history = model.fit(x_train, y_train,\n","            batch_size=batch_size,\n","            epochs=epochs,\n","            verbose=1,\n","            validation_data=(x_test, y_test))\n","\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Validation loss:', score[0])\n","print('Validation accuracy:', score[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PoNLXZvAt_Hr"},"source":["In machine learning it is always a good idea to pick a simple **baseline model** that you can compare your own models to. We will refer to the classifier above as our baseline. You should expect the validation accuracy of our baseline model to be around 90% after training for 10 epochs."]},{"cell_type":"markdown","metadata":{"id":"q_r7oO_Pm8TK"},"source":["###Model evaluation\n","For several reasons you always want to monitor how your model performs during training. The simplest way to monitor the training process is by plotting the loss and accuracy curves. Here we are doing it post-training, but there are tools that allow you to monitor the curves in real-time (see for instance [TensorBoard](https://www.tensorflow.org/tensorboard))."]},{"cell_type":"code","metadata":{"id":"gj2-tNJShfjj"},"source":["import matplotlib.pyplot as plt\n","\n","def show_history(history):\n","  plt.figure(figsize=(20,6))\n","\n","  # summarize history for accuracy\n","  plt.subplot(121)\n","  plt.plot(history.history['accuracy'])\n","  plt.plot(history.history['val_accuracy'])\n","  plt.title('model accuracy')\n","  plt.ylabel('accuracy')\n","  plt.xlabel('epoch')\n","  plt.legend(['train', 'validation'], loc='upper left')\n","\n","  # summarize history for loss\n","  plt.subplot(122)\n","  plt.plot(history.history['loss'])\n","  plt.plot(history.history['val_loss'])\n","  plt.title('model loss')\n","  plt.ylabel('loss')\n","  plt.xlabel('epoch')\n","  plt.legend(['train', 'validation'], loc='upper left')\n","  plt.show()\n","\n","show_history(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zDUhPEQcvwyw"},"source":["Examples of things you should be looking for:\n","\n","- The loss should go down over time.\n"," - If it increases, it usually means that your learning rate was set too high.\n"," - If it doesn't decrease, it could indicate that your learning rate was set too low.\n","- If the validation loss starts increasing, while the training loss is still decreasing, it means that your model has started overfitting.\n"," - Can you explain why?\n","- Simply stated, your model is done training when the loss curves hit a low plateau.\n"," - Are we done training in the above example, or do you think we should we be training for more epochs?\n"]},{"cell_type":"markdown","metadata":{"id":"U8v3SUB2o-Tp"},"source":["###Task 2.1: Batch size and number of epochs\n","Training on the entire MNIST training data set is guaranteed to work (almost) always. Simply because of the large number of images in the training set. This is boring. So let's make our problem a little more challenging by reducing the number of training examples to just 10 samples from each class."]},{"cell_type":"code","metadata":{"id":"wxOTzj5tp46I"},"source":["import numpy as np\n","\n","np.random.seed(0) # make it deterministic\n","\n","# create smaller training set\n","digit_indices = np.asarray([np.where(np.argmax(y_train,axis=1) == i)[0][np.random.randint(0,5000,10)] for i in range(num_classes)]).flatten()\n","x_train_small = x_train[digit_indices,:]\n","y_train_small = y_train[digit_indices,:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note:** Set verbose=1 to print output while training."],"metadata":{"id":"3oymSEIUHMCX"}},{"cell_type":"code","metadata":{"id":"5ZVxQgYIIG7Y"},"source":["import time\n","\n","batch_size = 5 # Cannot use 128 like before, because we only have 100 samples (10 from each class)\n","epochs = 20 # We need more epochs because we have fewer training samples\n","\n","tensorflow.random.set_seed(0)\n","\n","# Model (redefine the model in order to reinitialize the weights to random values)\n","model = Sequential()\n","model.add(Flatten(input_shape=input_shape))\n","model.add(Dense(num_classes))\n","model.add(Activation('softmax'))\n","\n","# Compile the model before training\n","model.compile(optimizer=keras.optimizers.SGD(),loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","# Fit model (this will take a little while. Set verbose to 1 if you want to see how training progresses)\n","start_time = time.time()\n","history = model.fit(x_train_small, y_train_small,\n","            batch_size=batch_size,\n","            epochs=epochs,\n","            validation_data=(x_test, y_test),\n","            verbose=0,\n","            shuffle=True)\n","print(\"--- training tool %s seconds ---\" % (time.time() - start_time))\n","\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Validation loss:', score[0])\n","print('Validation accuracy:', score[1])\n","\n","# Plot old vs new loss\n","show_history(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-0d5XHi8xNVL"},"source":["**Note:** Although we didn't change the underlying equation of our baseline model (`y = softmax(Wx+b)`), we did change the nature of the data on which the model was trained, and we also lowered the batch size and increased the number of epochs. So this is considered a **new model** that we could compare to our baseline model."]},{"cell_type":"markdown","metadata":{"id":"nkmF52862hxn"},"source":["###Question 2.2\n","1. Our new model performs worse than the baseline model. Why?\n","2. This puzzle is supposed to make you speculate. You are not required to get the answers right at this stage of the course. Temporarily set the number of epochs to 10 to make experimentation faster. How does training behave when we set the batch size to the values listed below? Compare the loss curves and the training times and see if you can figure out why they differ the way they do. Use these batch sizes:\n"," - 1 (lowest possible)\n"," - 5 (low)\n"," - 20 (medium)\n"," - 100 (highest possible)\n","3. Above, we changed the batch size and the number of epochs. Are these *hyperparameters* or *learnable parameters*? Btw., what are the learnable parameters of our model (``y = softmax(W*x + b)``)?\n","4. Which criteria would you pick to determine the \"optimal\" combination of batch size and number of epochs?"]},{"cell_type":"markdown","metadata":{"id":"jypf_eK3Iopc"},"source":["###Task 2.2: Finding a better learning rate\n","The learning rate is another hyperparameter that we can tweak.\n","\n","Training with the default learning rate (which is 0.01) is rather slow. Your task is to find a better learning rate that makes the model converge faster, without comprimising the model's accuracy on the validation set. With a proper learning rate you should be able to achieve 74-75% accuracy in just 10 epochs (instead of 20 as above).\n","\n","You can adjust the learning rate by setting the `learning_rate` argument of keras.optimizer.SGD:\n","\n","```\n","keras.optimizers.SGD(learning_rate=0.01)\n","```\n","\n","**Optional bonus questions:** What happens if you set the learning rate way too low (e.g. 0.0001)?\n"]},{"cell_type":"markdown","metadata":{"id":"bTIMPPIXnmps"},"source":["###Task 2.3: Displaying the learned weights\n","In the case of softmax regression there is a very intuitive interpretation of the learned weights of the coefficient matrix `W`, as you will see below.\n","\n","First, your task is to extract the weights of the coefficient matrix from the model (i.e. the `Dense` layer) and display each row as an image."]},{"cell_type":"code","metadata":{"id":"eTDJCvLqn_3n"},"source":["W = # Your code goes here\n","W = W.reshape((28,28,10)) # there are 10 classes and one 28x28 weight image per class\n","plt.figure(figsize=(20,4))\n","for i in range(10):\n","  ax = plt.subplot(2,5,i+1)\n","  plt.imshow(W[:,:,i])\n","  ax.get_xaxis().set_visible(False)\n","  ax.get_yaxis().set_visible(False)\n","  plt.gray()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SCMuJthmJCoj"},"source":["### Questions 2.3\n","1. How do we interpret the weights?\n","2. In some regions the weights are very noisy. What could be the consequences of that?"]},{"cell_type":"markdown","metadata":{"id":"d_HLWIZ7MUms"},"source":["###Task 2.4: Weight decay (or L2 regularization)\n","With only 10 observations per class in our training data set, it is very likely that our model overfits the training data. This leads to poor generalization (i.e., the model doesn't work that well on unseen data).\n","\n","One way to address overfitting is by means of regularization. The best kind of regularization is \"adding more data\" (of course). This is the reason that our baseline model performs better than the current model.\n","\n","One possible regularization strategy is to use *weight decay*. So let's modify the loss function of the model by adding an L2 regularization term. The regularization term is added using an extra parameter to the Dense layer.\n","\n","Please note that the weight of the penalty term (`lamda`) has been set rather high in the example below. As a consequence, you will actually see a small decrease in validation accuracy. But as you will see later, weight decay has dramatic effect on the learned weights (`W`). By the way, `lambda` is yet another hyperparameter that we could tweak to make our model perform better."]},{"cell_type":"code","metadata":{"id":"uiY_I0FKLmBv"},"source":["batch_size = 5\n","epochs = 20\n","\n","tensorflow.random.set_seed(0)\n","\n","# lamda is the weight of the L2 penalty term\n","lamda = 0.1\n","L2_regularizer = keras.regularizers.l2(lamda)\n","\n","# Model\n","model = Sequential()\n","model.add(Flatten(input_shape=input_shape))\n","model.add(Dense(num_classes,\n","                activation='softmax',\n","                kernel_regularizer=L2_regularizer))\n","\n","# Training\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.1),loss='categorical_crossentropy',metrics=['accuracy'])\n","history_reg = model.fit(x_train_small, y_train_small,\n","            batch_size=batch_size,\n","            epochs=epochs,\n","            verbose=0,\n","            validation_data=(x_test, y_test))\n","\n","# Evaluation\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Validation loss:', score[0])\n","print('Validation accuracy:', score[1])\n","\n","# Plot loss\n","print(\"Loss curves with regularization\")\n","show_history(history_reg)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-OigocSPpfuo"},"source":["**Now comes the interesting part:** Try once more to display the weights - this time of the regularized model."]},{"cell_type":"code","metadata":{"id":"4Ry1Am1ddFV9"},"source":["#W = # Your code goes here\n","W = W.reshape((28,28,10))\n","plt.figure(figsize=(20,4))\n","for i in range(10):\n","  ax = plt.subplot(2,5,i+1)\n","  plt.imshow(W[:,:,i])\n","  ax.get_xaxis().set_visible(False)\n","  ax.get_yaxis().set_visible(False)\n","  plt.gray()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1rivkoCuppm3"},"source":["###Questions 2.4\n","1. How do the weights of the regularized model differ from the weights of the non-regularized model?\n","2. Can you explain why?"]},{"cell_type":"markdown","metadata":{"id":"_Y2gg6tWesR1"},"source":["###Functional API instead of Sequential API\n","The models above have been specified using Keras' [Sequential API](https://keras.io/getting-started/sequential-model-guide/). Keras also allows you to specify models using the [Functional API](https://keras.io/getting-started/functional-api-guide/). The Keras functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.\n","\n","Here is how to set up the (baseline) softmax regression model using the functional API:"]},{"cell_type":"code","metadata":{"id":"rZ3KOeSjpoTJ"},"source":["from keras.layers import Input\n","from keras.models import Model\n","\n","tensorflow.random.set_seed(0)\n","\n","# This returns a tensor\n","inputs = Input(shape=input_shape)\n","\n","# A layer instance is callable on a tensor, and returns a tensor\n","x = Flatten()(inputs)\n","x = Dense(num_classes)(x)\n","predictions = Activation('softmax')(x)\n","\n","# This creates a model that includes the Input layer and the prediction layer\n","model = Model(inputs=inputs, outputs=predictions)\n","\n","# Training\n","model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n","model.fit(x_train, y_train,\n","          batch_size=128,\n","          epochs=10,\n","          verbose=1,\n","          validation_data=(x_test, y_test))\n","\n","# Evaluation\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Validation loss:', score[0])\n","print('Validation accuracy:', score[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zHf86VZYlrAB"},"source":["##Task 3: Our first CNN\n","It's time to move on and build our first CNN.\n","\n","Here is a simple example:"]},{"cell_type":"code","metadata":{"id":"wvHAmMda0uNE"},"source":["from keras.layers import Dropout\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.layers import Input\n","from keras.models import Model\n","\n","tensorflow.random.set_seed(0)\n","\n","inputs = Input(shape=(28, 28, 1))\n","\n","# Encoder (convolutional base)\n","x = Conv2D(filters=8, kernel_size=(3, 3), activation='relu')(inputs)\n","x = MaxPooling2D(pool_size=(2, 2))(x)\n","x = Conv2D(filters=16, kernel_size=(3, 3), activation='relu')(x)\n","x = MaxPooling2D(pool_size=(2, 2))(x)\n","x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(x)\n","encoded = Flatten()(x)\n","\n","# Decoder (2 fully connected layers)\n","x = Dense(units=64, activation='relu')(encoded)\n","x = Dropout(rate=0.5)(x)\n","predictions = Dense(units=num_classes,activation='softmax')(x)\n","\n","# This creates a callable model that includes the Input layer and the prediction layer\n","model = Model(inputs=inputs, outputs=predictions)\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SNATzsjAnWCi"},"source":["###Questions 3.1\n","1. How many layers does this CNN have?\n","2. How many convolution filters are there in the first convolution layer, and what is the width and height of the filters?\n","3. What does the MaxPooling2D layer do?\n","4. What does the Dropout layer do?\n","5. What is the shape of the input of the last convolution layer (i.e., just before the flatten layer)\n","\n","**Recall**\n","\n","You can make small experiments like this one\n","\n","```\n","input_shape = (1, 28, 28, 1) # Dimensions are (batch_size, height, width, num_channels)\n","x = tensorflow.random.normal(input_shape)\n","y = keras.layers.Conv2D(filters=2, kernel_size=3, activation='relu', padding=\"same\", input_shape=input_shape[1:])(x)\n","print(y.shape)\n","```\n","\n","**Optional bonus task:** See if you can figure out a way to display the filters of the first convolution layer as images."]},{"cell_type":"markdown","metadata":{"id":"5zgpNVDP2nKi"},"source":["For the record, the same model can also be defined using the sequential API:\n","\n","\n","```\n","from keras.layers import Dropout\n","from keras.layers import Conv2D ![alt text](https://), MaxPooling2D\n","\n","input_img = Input(shape=(28, 28, 1))\n","model = Sequential()\n","model.add(Conv2D(8,\n","                 kernel_size=(3, 3),\n","                 activation='relu',\n","                 input_shape=input_shape))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Conv2D(16,\n","                 kernel_size=(3, 3),\n","                 activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Conv2D(32,\n","                 kernel_size=(3, 3),\n","                 activation='relu'))\n","model.add(Flatten())\n","model.add(Dense(64,\n","                activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","model.summary()\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"44dU5LYWrMXY"},"source":["###Training\n","Let's train our model on the small dataset and evaluate it on the validation set:"]},{"cell_type":"code","metadata":{"id":"tB02sLW7mbX6"},"source":["model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.1),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","history = model.fit(x_train_small, y_train_small,\n","                    batch_size=10,\n","                    epochs=30,\n","                    verbose=1,\n","                    validation_data=(x_test, y_test))\n","\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Validation loss:', score[0])\n","print('Valdiation accuracy:', score[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H_1w3A-jr0od"},"source":["show_history(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bnRuBQORVrQy"},"source":["Notice how this model is able to achieve a higher validation accuracy on the small training dataset (before we got 74.4%, now we have 80%). It does come at an expense, however; the CNN model needs to train for longer, having to do with the fact that it has more layers and more learnable parameters.\n","\n","**Note:** If you train the above CNN model on the full MNIST dataset, you can expect to get at least 97-98% validation accuracy. That is quite an improvement over our baseline model (softmax regression), which got 89%."]},{"cell_type":"markdown","metadata":{"id":"jOxQ0LletyuB"},"source":["###Task 3.1 Encoding the MNIST dataset in 2 dimensions with a CNN\n","In the above model the input shape to the Flatten layer  is 3x3x32, which is then flattened to a 288-dimensional vector, corresponding to the variable named `encoded`.\n","\n","Your task is to modify the network such that variable `encoded` has dimensionality 2 instead of 288.\n","\n","*Hint:* You could insert an extra layer before the Flatten layer that reduces the 3x3x32 input tensor to a 1x1x2 tensor. There are several solutions. It might be a good idea to add an activation function to the layer that you add. I used `activation='tanh'`. I will explain in the solution why this is a good idea."]},{"cell_type":"code","metadata":{"id":"nF0XJUxJ4Qpj"},"source":["from keras.layers import Dropout\n","from keras.layers import Conv2D, MaxPooling2D\n","\n","tensorflow.random.set_seed(0)\n","\n","inputs = Input(shape=(28, 28, 1))\n","\n","# Encoder (convolutional base)\n","x = Conv2D(8, kernel_size=(3, 3), activation='relu')(inputs)\n","x = MaxPooling2D((2, 2))(x)\n","x = Conv2D(16, kernel_size=(3, 3), activation='relu')(x)\n","x = MaxPooling2D((2, 2))(x)\n","x = Conv2D(32, kernel_size=(3, 3), activation='relu')(x)\n","x = # Your code goes here\n","encoded = Flatten()(x)\n","\n","# Decoder (2 fully connected layers)\n","x = Dense(64, activation='relu')(encoded)\n","x = Dropout(0.5)(x)\n","predictions = Dense(num_classes,activation='softmax')(x)\n","\n","# This creates a callable model that includes the Input layer and the prediction layer\n","model = Model(inputs=inputs, outputs=predictions)\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"69KNBxBEv53g"},"source":["###Training\n","This time we train the model on the full dataset, because compressing the images to 2 dimensions is a difficult task that requires more training data."]},{"cell_type":"code","metadata":{"id":"eyDbES-izBol"},"source":["model.compile(loss=keras.losses.categorical_crossentropy,\n","              optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train,\n","          batch_size=100,\n","          epochs=10,\n","          verbose=1,\n","          validation_data=(x_test, y_test))\n","\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9HrN8wbHwA0q"},"source":["###Task 3.2\n","Run all test examples (`x_test`) through your model and for each example extract the 2-dimensional vector output of the Flatten layer (variable named `encoded`). To make predictions on a batch of images, you can do like this: ``out = model_encoded.predict(batch)``.\n","\n","Then plot those vectors in a 2D plot, where each class gets its own color.\n","\n","You might find this code useful:"]},{"cell_type":"code","metadata":{"id":"AGUoaLHVwn5T"},"source":["model_encoded = Model(inputs=model.input, outputs=encoded)\n","\n","# Plot 10 dots with 10 different colors\n","for i in range(10):\n","  plt.plot(i,i,'.C'+str(i))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9iR1BvO15aCQ"},"source":["###Comment\n","The point of this little exercise is really to illustrate the power of CNNs as image encoders. Think of what we have just accompplished: We have embedded 28x28=784 dimensional images/vectors down to just 2 dimensions, and we can still distinguish the 10 classes from each other! This is a really powerful concept for visualizing high-dimensional data in 2D."]},{"cell_type":"markdown","metadata":{"id":"6YJnv8WI7b4B"},"source":["##Task 4: Convolutional Autoencoder\n","Autoencoders are special types of neural networks that map the input X to the same output (namely X). So the autoencoder (AE) is an identity function:\n","\n","```\n","X = AE(X)\n","```\n","\n","So what's the point? The point is that the autoencoder compresses the image down to a low-dimensional representation, which can be decoded again to reconstruct the original input image. This has many useful applications, such as data compression and representation learning. Only the important information is stored in the low-dimensional representation.\n","\n","The autoencoder consists of a trained encoder (E) and a trained decoder (D):\n","\n","```\n","X = AE(X) = D(E(X))\n","```\n","\n","It is typically (but not always) the encoding E(X) that we are interested in.\n","\n","Note that the autoecoder does not need the class labels to train. So it is an **unsupervised** machine learning technique.\n","\n","Here is an example of a convolutional autoencoder:"]},{"cell_type":"code","metadata":{"id":"KNUtSwB-1thF"},"source":["from keras.layers import UpSampling2D\n","\n","tensorflow.random.set_seed(0)\n","inputs = Input(shape=(28, 28, 1))\n","\n","# Encoder (convolutional base)\n","x = Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n","x = MaxPooling2D((2, 2), padding='same')(x)\n","x = Conv2D(8, kernel_size=(3, 3), activation='relu', padding='same')(x)\n","x = MaxPooling2D((2, 2), padding='same')(x)\n","x = Conv2D(8, kernel_size=(3, 3), activation='relu', padding='same')(x)\n","encoded = MaxPooling2D((2, 2), padding='same')(x)\n","print((\"shape of encoded\", K.int_shape(encoded)))\n","\n","# Decoder (upsamling)\n","x = Conv2D(8, kernel_size=(3, 3), activation='relu', padding='same')(encoded)\n","x = UpSampling2D((2, 2))(x)\n","x = Conv2D(8, kernel_size=(3, 3), activation='relu', padding='same')(x)\n","x = UpSampling2D((2, 2))(x)\n","x = Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n","x = UpSampling2D((2, 2))(x)\n","decoded = Conv2D(1, kernel_size=(5, 5), padding='valid')(x)\n","print((\"shape of decoded\", K.int_shape(decoded)))\n","\n","autoencoder = Model(inputs, decoded)\n","autoencoder.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nk0IV2fFAfuw"},"source":["###Questions 4.1\n","1. What is the shape of the encoded image?\n","2. Why are we not flattening (i.e., vectorizing) the encoded image like we did in the CNN classifier above?\n","3. What does UpSampling2D do?\n","4. Why do you think upsampling is followed by a convolution? Hint: It has to do with the way we are upsampling...\n","5. See if you can figure out what `padding` means. What is the difference between using `padding='same'` and `padding='valid'`?\n","\n","**Again** remember that you can carry out small experiments to get the answers."]},{"cell_type":"markdown","metadata":{"id":"bKXQlf8tBnmc"},"source":["###Training\n","Let's train the autoencoder:"]},{"cell_type":"code","metadata":{"id":"Vw_OqAHD5k1-"},"source":["autoencoder.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9), loss='mse')\n","autoencoder.fit(x_train, x_train, epochs=10, batch_size=100,\n","               shuffle=True, validation_data=(x_test, x_test), verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nfCKH2rMgSOi"},"source":["And show some test results"]},{"cell_type":"code","metadata":{"id":"edNyzCMW6PaS"},"source":["decoded_imgs = autoencoder.predict(x_test)\n","print(\"input (upper row)\")\n","show_imgs(x_test)\n","print(\"decoded (bottom row)\")\n","show_imgs(decoded_imgs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_LAsmAerCSLh"},"source":["###Improving the autoencoder\n","This is okay, but not great. Let's use binary cross entropy (BCE) loss instead of mean squared error (MSE) loss. Using BCE assumes that the inputs are a probabilities, hence the sigmoid below to squash the values in between 0 and 1."]},{"cell_type":"code","metadata":{"id":"nJJhiPNeCiAg"},"source":["decoded_sigmoid = Activation('sigmoid')(decoded) # decoded is the output of the first autoencoder\n","autoencoder2 = Model(inputs, decoded_sigmoid)\n","autoencoder2.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9), loss='binary_crossentropy')\n","autoencoder2.fit(x_train, x_train, epochs=10, batch_size=100,\n","               shuffle=True, validation_data=(x_test, x_test), verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OyRb-DNXDBBM"},"source":["decoded_imgs = autoencoder2.predict(x_test)\n","print(\"input (upper row)\")\n","show_imgs(x_test)\n","print(\"decoded (bottom row)\")\n","show_imgs(decoded_imgs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SYNuOKadDk_P"},"source":["###Questions 4.2\n","1. What is the difference between 'mse' loss and 'binary_crossentropy' loss?\n","2. Can you explain why 'binary_crossentropy' works better?"]},{"cell_type":"markdown","metadata":{"id":"LEU1ZDqg6bk9"},"source":["##Task 5: Denoising Autoencoder\n","Autoencoders can get really advanced, like [Variational Autoencoders](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf). A slightly less complicated, yet powerful autoecoder variant is the [Denoising Autoencoder](https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf).\n","\n","As stated above autoencoders have many useful applications. One of these is *noise reduction*. The underlying idea is very simple: Add random noise to the input X, and teach the autoencoder to remove the noise. That is, the autoencoder should learn the mapping:\n","\n","```\n","X = AE(X + noise)\n","```"]},{"cell_type":"markdown","metadata":{"id":"MnzL99s2IYtR"},"source":["###Task 5.1\n","Create two new data sets based on x_train and x_test, where you have added noise such that\n","\n","```\n","x_train_noisy = x_train + noise\n","x_test_noisy = x_test + noise\n","```\n","\n","You may want to look at numpy functions like [np.random.normal](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html) and [np.clip](https://numpy.org/doc/stable/reference/generated/numpy.clip.html).\n","\n","**Your tasks:**\n","\n","1. Does ``autoencoder2`` work on the noisy images, ``x_test_noisy``? (test it!)\n","2. Retrain ``autoencoder2`` on the noisy images (input = ``x_train_noisy`` and output = ``x_train``). What do you observe?\n","\n","How much noise can you add before the autoencoder breaks down (fails to remove the noise)?"]},{"cell_type":"code","metadata":{"id":"wD06QF9EwDbp"},"source":["#Your code goes here\n","#x_train_noisy = ???\n","#x_test_noisy = ???"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0-16HefOKrJF"},"source":["Let's first verify that the existing autoencoder doesn't work well on noisy input images:"]},{"cell_type":"code","metadata":{"id":"1vH7PBypK2WQ"},"source":["# denoising\n","print(\"denoising\")\n","decoded_imgs = autoencoder2.predict(x_test_noisy)\n","print(\"input (upper row)\")\n","show_imgs(x_test_noisy)\n","print(\"decoded (bottom row)\")\n","show_imgs(decoded_imgs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G-ENwW7aLL36"},"source":["Doesn't look that good. Let's see if we can teach the model to remove the noise by showing it training images that contain noise. Instead of training from scratch, we will *fine-tune* the model that has already been trained:"]},{"cell_type":"code","metadata":{"id":"YH5iYSws6nI0"},"source":["# it takes more epochs to converge\n","autoencoder2.fit(x_train_noisy, x_train, epochs=10, batch_size=100,\n","                shuffle=True, validation_data=(x_test_noisy, x_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ynvpaB666wlm"},"source":["# denoising\n","print(\"denoising\")\n","decoded_imgs = autoencoder2.predict(x_test_noisy)\n","print(\"input (upper row)\")\n","show_imgs(x_test_noisy)\n","print(\"decoded (bottom row)\")\n","show_imgs(decoded_imgs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Zoir9fpLjpK"},"source":["Pretty cool, right?\n","\n","Of cause the updated autoencoder still works on the noise-free images:"]},{"cell_type":"code","metadata":{"id":"IbNotnbQLnqP"},"source":["# what if we feed the original noise-free test images?\n","decoded_imgs = autoencoder2.predict(x_test)\n","print(\"\\nof course, it works with original noise-less images\")\n","print(\"input (upper row)\")\n","show_imgs(x_test)\n","print(\"decoded (bottom row)\")\n","show_imgs(decoded_imgs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F1cRW2XrKHif"},"source":["##Task 6: Super resolution\n","The convolutional autoencoder is a network that maps an image to another image. There are other types of these *image-to-image networks*.\n","\n","One example is a super resolution network. This is pretty much an autoencoder, except that the input image has lower spatial resolution than the output image. Super resolution networks learn to increase the spatial of the input image.\n","\n","Your task is to modify the autoencoder such that it takes an 14x14x1 image as input and transforms it to a 28x28x1 image. Specifically, the training and test inputs should be\n","\n"]},{"cell_type":"code","metadata":{"id":"Dvcl2mGLu0v3"},"source":["# Low resolution images (pick every other pixel)\n","x_train_lowres = x_train[:,::2,::2,:] # 14x14x1\n","x_test_lowres = x_test[:,::2,::2,:] # 14x14x1\n","\n","# Show example\n","print(\"Input low resolution images\")\n","show_imgs(x_train_lowres)\n","print(\"Output high resolution images (target)\")\n","show_imgs(x_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SjUB_cm4yRj6"},"source":["##Task 7: Image regression\n","Recall that in a regression problems the output of the model is one or more scalar values, rather than class labels. Both the autoencoder and super resolution network are examples of regression models. Another example of image regression is [facial landmark prediction](https://medium.com/@rishiswethan.c.r/emotion-detection-using-facial-landmarks-and-deep-learning-b7f54fe551bf), which can be used for emotion recognition.\n","\n","In this task we will estimate the rotation angle of rotated MNIST images (but it could just as well have been estimating pixel coordinates of facial landmarks).\n","\n","As a first step, we need an image generator that generates batches of randomly rotated images, along with the target rotation angles that the model should learn to predict. This code was modified from https://d4nst.github.io/2017/01/12/image-orientation/\n","\n","**Optional bonus task:** Writing your own custom data generators is a common task in deep learning. Once you have completed the main task, consider going through the code of  `RotNetDataGenerator` and see if you can figure out how it works (notice that its base class is [`keras.preprocessing.image.Iterator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/Iterator))."]},{"cell_type":"code","metadata":{"id":"EAhmL_jj_5m-"},"source":["from keras.preprocessing.image import Iterator\n","from keras.utils.np_utils import to_categorical\n","import cv2\n","\n","class RotNetDataGenerator(Iterator):\n","\n","    def __init__(self, input, batch_size=64,\n","                 preprocess_func=None, shuffle=False):\n","\n","        self.images = input\n","        self.batch_size = batch_size\n","        self.input_shape = self.images.shape[1:]\n","        self.preprocess_func = preprocess_func\n","        self.shuffle = shuffle\n","\n","        # add dimension if the images are greyscale\n","        if len(self.input_shape) == 2:\n","            self.input_shape = self.input_shape + (1,)\n","        N = self.images.shape[0]\n","\n","        super(RotNetDataGenerator, self).__init__(N, batch_size, shuffle, None)\n","\n","    def _get_batches_of_transformed_samples(self, index_array):\n","        # create array to hold the images\n","        batch_x = np.zeros((len(index_array),) + self.input_shape, dtype='float32')\n","        # create array to hold the labels\n","        batch_y = np.zeros(len(index_array), dtype='float32')\n","\n","        # iterate through the current batch\n","        for i, j in enumerate(index_array):\n","\n","            image = self.images[j].squeeze()\n","\n","            # get a random angle\n","            rotation_angle = np.random.randint(-30,30)\n","\n","            # rotate the image\n","            rows,cols = image.shape\n","            M = cv2.getRotationMatrix2D(((cols-1)/2.0,(rows-1)/2.0),rotation_angle,1)\n","            rotated_image = cv2.warpAffine(image,M,(cols,rows))\n","\n","            # add dimension to account for the channels if the image is greyscale\n","            if rotated_image.ndim == 2:\n","                rotated_image = np.expand_dims(rotated_image, axis=2)\n","\n","            # store the image and label in their corresponding batches\n","            batch_x[i] = rotated_image\n","            batch_y[i] = rotation_angle\n","\n","        # preprocess input images\n","        if self.preprocess_func:\n","            batch_x = self.preprocess_func(batch_x)\n","\n","        return batch_x, batch_y\n","\n","    def next(self):\n","        with self.lock:\n","            # get input data index and size of the current batch\n","            index_array = next(self.index_generator)\n","        # create array to hold the images\n","        return self._get_batches_of_transformed_samples(index_array)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PBRKLikE37WE"},"source":["###Test the generator"]},{"cell_type":"code","metadata":{"id":"LJSiRdxcBcEQ"},"source":["# Instantiate\n","datagen = RotNetDataGenerator(\n","        x_train,\n","        batch_size=32,\n","        preprocess_func=None,\n","        shuffle=False\n","    )\n","\n","# Generate batch\n","rotated_images, angles = datagen.next()\n","\n","# Display\n","print(\"Images (before rotation)\")\n","show_imgs(x_train)\n","print(\"Images after random rotation\")\n","show_imgs(rotated_images)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-rWkMWvh9hNt"},"source":["###Task 7.1\n","Make a small CNN that takes as input an 28x28x1 image and outputs a single scalar value (the rotation angle).\n","\n","The last layer of your network should be\n","\n","```\n","angle = Dense(1)(x)\n","```\n","\n","Note that this is a dense layer **without** any activation function, hence the output of this layer is simply `angle = W*x + b`."]},{"cell_type":"code","metadata":{"id":"f3EMvU5N94L3"},"source":["inputs = Input(shape=(28, 28, 1))\n","\n","# Encoder (convolutional base)\n","# Your codes goes here\n","\n","# Decoder (predict angle)\n","x = # Your code goes here\n","angle = Dense(1)(x)\n","\n","angle_estimator = Model(input=inputs, output=angle)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H9eeVPFI-tVq"},"source":["###Training"]},{"cell_type":"code","metadata":{"id":"bW2RzBPJG0P-"},"source":["angle_estimator.compile(optimizer='rmsprop',loss='mse')\n","\n","# training loop\n","angle_estimator.fit_generator(\n","    RotNetDataGenerator(\n","        x_train,\n","        batch_size=100,\n","        preprocess_func=None,\n","        shuffle=True\n","    ),\n","    epochs=50,\n","    validation_data=RotNetDataGenerator(\n","        x_test,\n","        batch_size=100,\n","        preprocess_func=None))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"98or9_7_6YNu"},"source":["###Model evaluation"]},{"cell_type":"code","metadata":{"id":"jYmyHlZ5MEKH"},"source":["# Set up generator\n","datagen = RotNetDataGenerator(\n","        x_test,\n","        batch_size=32,\n","        preprocess_func=None,\n","        shuffle=False\n","    )\n","\n","# Generate test images\n","rotated_images, angles = datagen.next()\n","print(\"Test images before rotation\")\n","show_imgs(x_test)\n","print(\"Test images after rotation\")\n","show_imgs(rotated_images)\n","\n","# Predict angles\n","angles_pred = angle_estimator.predict(rotated_images)\n","\n","# Plot angles\n","print('Predicted vs. true rotation angles')\n","plt.plot(angles)\n","plt.plot(angles_pred)\n","plt.legend(['True','Predicted']);\n","plt.xlabel('Test image')\n","plt.ylabel('Rotation angle')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dqtngpfbAaM8"},"source":["**HELP**: The predicted angles should match the true angles reasonably well. If your model fails to predict the angles, it could be because the model is underfitting. This indicates that the capacity of the model is too low. To increase capacity, add more connections in dense layers and/or more output maps in convolutional layers (I will explain this in the class)."]},{"cell_type":"markdown","metadata":{"id":"xLrZfojj8Cd7"},"source":["###De-rotate images\n","Now that we have estimated the rotation angles, let's de-rotate the images back to their original alignment."]},{"cell_type":"code","metadata":{"id":"H78jSdQoNC5-"},"source":["de_rotated_images = np.zeros(rotated_images.shape)\n","\n","for i in range(rotated_images.shape[0]):\n","  image = rotated_images[i,:,:,:].squeeze()\n","\n","  # get predicted angle\n","  rotation_angle = -angles_pred[i]\n","\n","  # rotate the image\n","  rows,cols = image.shape\n","  M = cv2.getRotationMatrix2D(((cols-1)/2.0,(rows-1)/2.0),float(rotation_angle),1)\n","  de_rotated_image = cv2.warpAffine(image,M,(cols,rows))\n","\n","  de_rotated_images[i,:,:,0] = de_rotated_image\n","\n","print('Images before rotation (ground truth)')\n","show_imgs(x_test)\n","print('Images after rotation (to be de-rotated)')\n","show_imgs(rotated_images)\n","print('De-rotated images (should match ground truth)')\n","show_imgs(de_rotated_images)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NV6vSV3WCQZd"},"source":["##Task 8: Object detection\n","**Motivation:** Classification CNNs assign one label to each input image. This is problematic if the image contains multiple objects.\n","\n","Object detection is about detecting and classifying multiple objects in images. Object detection networks output the corner coordinates of the bounding box of each detect object, along with a class label.\n","\n","There are many ways to implement object detection with CNNs. At some point you may want to take a look at this 3-part tutorial:\n","- https://towardsdatascience.com/beginners-guide-to-object-detection-algorithms-6620fb31c375\n","- https://www.analyticsvidhya.com/blog/2018/11/implementation-faster-r-cnn-python-object-detection/?utm_source=blog&utm_medium=a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1\n","- https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/?utm_source=blog&utm_medium=implementation-faster-r-cnn-python-object-detection\n","\n","The basic idea of our simple object detector below is as follows:\n","\n","- The output image is divided into a 2-by-2 grid\n","- Each grid cell can contain one object, or no object. We want our model to output whether it thinks there is an object in the cell or not.\n","- If a cell contains an object, we want our model to output the corner coordinates of the bounding box (relative to the center of the grid cell).\n","- Also, if there is an object in a cell, we want to predict its class label.\n","\n","If the input image has shape 64x64, the output of the model will have shape 2x2x15:\n","- 1 output per cell for the confidence (is there an object or not?)\n","- 4 outputs per cell corresponding to the upper left and lower right coordinates of the bounding box\n","- 10 outputs per cell corresponding to the class probabalities (assuming we have 10 classes).\n","\n","This totals 15 outputs per cell.\n"]},{"cell_type":"markdown","metadata":{"id":"Jye_-hKPONRQ"},"source":["###Comment\n","The network architecture below is somewhat inspired by YOLO:\n","\n","![alt text](https://miro.medium.com/max/1152/1*m8p5lhWdFDdapEFa2zUtIA.jpeg)"]},{"cell_type":"markdown","metadata":{"id":"rHkjYyA09Slk"},"source":["###Task 8.1\n","Fill in the empty slots below (marked with ???), then run the code block to set up the object detection network."]},{"cell_type":"code","metadata":{"id":"DBaW6otN9dHO"},"source":["from keras.layers import concatenate\n","from keras.activations import softmax\n","\n","def softMaxAxis3(x):\n","    return softmax(x,axis=3)\n","\n","input_img = Input(shape=(64, 64, 1))\n","\n","# Shared encoder\n","x = Conv2D(8, 33, activation='relu', padding='same')(input_img) #nb_filter, nb_row, nb_col\n","x = MaxPooling2D((2, 2), padding='same')(x)\n","x = Conv2D(16, 3, activation='relu', padding='same')(x)\n","x = MaxPooling2D((2, 2), padding='same')(x)\n","x = Conv2D(32, 3, activation='relu', padding='same')(x)\n","encoded = MaxPooling2D((2, 2), padding='same')(x)\n","\n","# Notice that all three outputs use the same (shared) encoder\n","\n","# 1. This predicts whether there is an object in a cell or not\n","x = Conv2D(4, 3, activation='relu', padding='same')(encoded)\n","x = MaxPooling2D((2, 2), padding='same')(x)\n","x = Conv2D(4, 3, activation='relu', padding='same')(x)\n","x = MaxPooling2D((2, 2), padding='same')(x)\n","confidence = Conv2D(???, 1, activation='sigmoid', padding='same')(x)\n","\n","# 2. This predicts the bounding box coordinates for each cell\n","x = Conv2D(4, 3, activation='relu', padding='same')(encoded)\n","x = MaxPooling2D((2, 2), padding='same')(x)\n","x = Conv2D(4, 3, activation='relu', padding='same')(x)\n","x = MaxPooling2D((2, 2), padding='same')(x)\n","box = Conv2D(???, 1, padding='same')(x)\n","\n","# 3. This predicts the class probabilities for each cell\n","x = Conv2D(16, 3, activation='relu', padding='same')(encoded)\n","x = MaxPooling2D((2, 2), padding='same')(x)\n","x = Conv2D(16, 3, activation='relu', padding='same')(x)\n","x = MaxPooling2D((2, 2), padding='same')(x)\n","classes = Conv2D(???, 1, activation=softMaxAxis3, padding='same')(x)\n","\n","# Merge output\n","merged = concatenate([confidence, box, classes])\n","\n","objdet = Model(input_img, merged)\n","objdet.compile(optimizer='rmsprop', loss='mse')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1FGcUFY18kOO"},"source":["###Quesions 8.1\n","1. What does softMaxAxis3 do, and why is it needed?"]},{"cell_type":"markdown","metadata":{"id":"VtLlialYGZVm"},"source":["###Training data\n","Let's generate some training data for our object detector.\n","\n","The training images (`x_train_obj`) will be 64x64, where two of the four quadrants will contain one handwritten digit. This just serves to illustrate that we can teach a network to detect and classify more than one digit per input image.\n","\n","The output is (`y_train_obj`) is 2x2x15 as explained above:\n","- 1 output per cell for the confidence (is there an object or not?)\n","- 4 outputs per cell corresponding to the upper left and lower right coordinates of the bounding box\n","- 10 outputs per cell corresponding to the class probabalities (assuming we have 10 classes).\n"]},{"cell_type":"code","metadata":{"id":"s8EGlKP8Gd28"},"source":["x_train_obj = np.zeros((5000,64,64,1))\n","y_train_obj = np.zeros((5000,2,2,15))\n","\n","for i in range(5000):\n","\n","  ## 1\n","  q = np.random.randint(0,2) # 1st or 2nd image quadrant?\n","\n","  # Random image\n","  rand_ix = np.random.randint(0,x_train.shape[0])\n","  x_off_start = np.random.randint(0,5)\n","  y_off_start = np.random.randint(0,5)\n","  x_train_obj[i,0+x_off_start:28+x_off_start,32*q+y_off_start:32*q+28+y_off_start,:] = x_train[rand_ix,:,:,:]\n","\n","  # Set confidence to 1\n","  y_train_obj[i,0,q,0] = 1\n","\n","  # Class label\n","  label = np.argmax(y_train[rand_ix,:])\n","  y_train_obj[i,0,q,5+label] = 1\n","\n","  # Bounding box corners coordinates\n","  tmp = x_train[rand_ix,:,:,:]\n","  ix = np.where(tmp>0.1)\n","  rows = ix[0]\n","  cols = ix[1]\n","  y_train_obj[i,0,q,1] = np.min(rows) + x_off_start\n","  y_train_obj[i,0,q,2] = np.min(cols) + y_off_start\n","  y_train_obj[i,0,q,3] = np.max(rows) + x_off_start\n","  y_train_obj[i,0,q,4] = np.max(cols) + y_off_start\n","\n","  ## 2\n","  q = np.random.randint(0,2) # 3rd or 4rd image quadrant?\n","\n","  # Random image\n","  rand_ix = np.random.randint(0,x_train.shape[0])\n","  x_off_start = np.random.randint(0,5)\n","  y_off_start = np.random.randint(0,5)\n","  x_train_obj[i,32+x_off_start:32+28+x_off_start,32*q+y_off_start:32*q+28+y_off_start,:] = x_train[rand_ix,:,:,:]\n","\n","  # Set confidence to 1\n","  y_train_obj[i,1,q,0] = 1\n","\n","  # Class label\n","  label = np.argmax(y_train[rand_ix,:])\n","  y_train_obj[i,1,q,5+label] = 1\n","\n","  # Bounding box corners coordinates\n","  tmp = x_train[rand_ix,:,:,:]\n","  ix = np.where(tmp>0.1)\n","  rows = ix[0]\n","  cols = ix[1]\n","  y_train_obj[i,1,q,1] = np.min(rows) + x_off_start\n","  y_train_obj[i,1,q,2] = np.min(cols) + y_off_start\n","  y_train_obj[i,1,q,3] = np.max(rows) + x_off_start\n","  y_train_obj[i,1,q,4] = np.max(cols) + y_off_start"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DLAKnAV0KkSC"},"source":["Display example outputs"]},{"cell_type":"code","metadata":{"id":"3Usjz_hbI7k4"},"source":["import cv2\n","plt.figure(figsize=(20,4))\n","for k in range(10):\n","  result = np.tile(x_train_obj[k,:,:,:],(1,1,3))\n","\n","  for i in range(2):\n","    for j in range(2):\n","      object_present = np.round(y_train_obj[k,i,j,0])\n","      if object_present:\n","        class_index = np.argmax(y_train_obj[k,i,j,5:])\n","        xmin = int(y_train_obj[k,i,j,1] + i*32) # row\n","        ymin = int(y_train_obj[k,i,j,2] + j*32) # col\n","        xmax = int(y_train_obj[k,i,j,3] + i*32) # row\n","        ymax = int(y_train_obj[k,i,j,4] + j*32) # col\n","        cv2.rectangle(result,(ymin,xmin),(ymax,xmax),(0,1,0),1)\n","        cv2.putText(result, str(class_index), (ymin-10, xmin+10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 1, 0),lineType=cv2.LINE_AA)\n","  ax = plt.subplot(2,5,k+1)\n","  plt.imshow(result)\n","  ax.get_xaxis().set_visible(False)\n","  ax.get_yaxis().set_visible(False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q3x93OVGOLvO"},"source":["###Training\n","We are ready to start training the model."]},{"cell_type":"code","metadata":{"id":"osyiFWyhWUri"},"source":["objdet.fit(x_train_obj, y_train_obj, epochs=100, batch_size=128,shuffle=True,verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TveS0ULsOR75"},"source":["###Model evaluation"]},{"cell_type":"code","metadata":{"id":"c1kETljRYh7f"},"source":["out = objdet.predict(x_train_obj[0:10,:,:,:])\n","\n","plt.figure(figsize=(20,4))\n","for k in range(10):\n","  result = np.tile(x_train_obj[k,:,:,:],(1,1,3))\n","\n","  for i in range(2):\n","    for j in range(2):\n","      object_present = np.round(out[k,i,j,0])\n","      if object_present:\n","        class_index = np.argmax(out[k,i,j,5:])\n","        xmin = int(out[k,i,j,1] + i*32) # row\n","        ymin = int(out[k,i,j,2] + j*32) # col\n","        xmax = int(out[k,i,j,3] + i*32) # row\n","        ymax = int(out[k,i,j,4] + j*32) # col\n","        cv2.rectangle(result,(ymin,xmin),(ymax,xmax),(0,1,0),1)\n","        cv2.putText(result, str(class_index), (ymin-10, xmin+10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 1, 0),lineType=cv2.LINE_AA)\n","  ax = plt.subplot(2,5,k+1)\n","  plt.imshow(result)\n","  ax.get_xaxis().set_visible(False)\n","  ax.get_yaxis().set_visible(False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sK3z-27PBVLn"},"source":["**Note:** Do not expect perfect results. Our object detection network is over-simplified compared to state-of-the-art. Or more precisely, the loss function is far from ideal, so we are optimizing *the wrong objective*, so to say."]},{"cell_type":"markdown","metadata":{"id":"QJzrjPFn455A"},"source":["##Task 9: Image segmentation\n","Image segmentation is a form of image-to-image transformation. It outputs a softmax classification per pixel. So if the input image has size 64x64, and there are 10 classes, the output will have shape 64x64x10. That is, for each pixel the network outputs a vector of class probabilities:\n","\n","![alt text](https://miro.medium.com/max/498/1*P1ooLjeSwhxeJGyFawCvaQ.png)\n","\n","Source of inspiration: https://medium.com/100-shades-of-machine-learning/https-medium-com-100-shades-of-machine-learning-rediscovering-semantic-segmentation-part1-83e1462e0805\n","\n","Let's first make a new training set."]},{"cell_type":"markdown","metadata":{"id":"qn4GeiUOLYe-"},"source":["###Training images\n","The training images (x_train_seg) will be 64x64, where two of the four quadrants will contain one handwritten digit. This is to illustrate that we can teach a network to identify and segment more than one digit per input image.\n","\n","The target output (`y_train_seg`) will be 64x64x10, with a one-hot vector for each pixel indicating the correct class."]},{"cell_type":"code","metadata":{"id":"5jS8Dwk6H2Qr"},"source":["x_train_seg = np.zeros((5000,64,64,1))\n","y_train_seg = np.zeros((5000,64,64,10))\n","\n","for i in range(5000):\n","\n","  ## 1\n","  q = np.random.randint(0,2) # 1st or 2nd image quadrant?\n","  rand_ix = np.random.randint(0,x_train.shape[0])\n","  x_off_start = np.random.randint(0,5)\n","  y_off_start = np.random.randint(0,5)\n","  x_train_seg[i,0+x_off_start:28+x_off_start,32*q+y_off_start:32*q+28+y_off_start,:] = x_train[rand_ix,:,:,:]\n","\n","  # Mask\n","  tmp = x_train[rand_ix,:,:,:]\n","  ix = np.where(tmp>0.1)\n","  tmp = np.zeros(tmp.shape)\n","  tmp[ix] = 1\n","  label = np.argmax(y_train[rand_ix,:])\n","  y_train_seg[i,0+x_off_start:28+x_off_start,32*q+y_off_start:32*q+28+y_off_start,label] = tmp.squeeze()\n","\n","  ## 2\n","  q = np.random.randint(0,2) #  3rd or 4th image quadrant?\n","  rand_ix = np.random.randint(0,x_train.shape[0])\n","  x_off_start = np.random.randint(0,5)\n","  y_off_start = np.random.randint(0,5)\n","  x_train_seg[i,32+x_off_start:32+28+x_off_start,32*q+y_off_start:32*q+28+y_off_start,:] = x_train[rand_ix,:,:,:]\n","\n","  # Mask\n","  tmp = x_train[rand_ix,:,:,:]\n","  ix = np.where(tmp>0.1)\n","  tmp = np.zeros(tmp.shape)\n","  tmp[ix] = 1\n","  label = np.argmax(y_train[rand_ix,:])\n","  y_train_seg[i,32+x_off_start:32+28+x_off_start,32*q+y_off_start:32*q+28+y_off_start,label] = tmp.squeeze()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sNh7RfpjM9rl"},"source":["# Show examples of input to output mappings\n","for ex in range(5):\n","  plt.figure(figsize=(20,6))\n","  rand_ix = np.random.randint(0,5000)\n","  ax = plt.subplot(1,11,1)\n","  plt.imshow(x_train_seg[rand_ix,:,:,:].squeeze())\n","  plt.gray()\n","  plt.title('Input image')\n","  for i in range(10):\n","    ax = plt.subplot(1,11,i+2)\n","    plt.imshow(y_train_seg[rand_ix,:,:,i].squeeze())\n","    plt.gray()\n","    plt.title(\"Out class \"+str(i))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sVfoiPrt_210"},"source":["###U-Net for image segmentation\n","We will use a light version of the so-called U-Net:\n","- https://arxiv.org/pdf/1505.04597.pdf\n","- https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47\n","\n","![alt text](https://miro.medium.com/max/720/1*OkUrpDD6I0FpugA_bbYBJQ.png)"]},{"cell_type":"code","metadata":{"id":"a_Yw6_PkkWkU"},"source":["from keras.layers import concatenate, BatchNormalization\n","\n","# See last layer of network\n","def softMaxAxis3(x):\n","    return softmax(x,axis=3)\n","\n","def my_conv(x,filters,kernel_size=3,padding='same',kernel_initializer='he_normal'):\n","  x = Conv2D(filters, kernel_size, padding=padding, kernel_initializer=kernel_initializer)(x)\n","  x = BatchNormalization()(x)\n","  x = Activation('relu')(x)\n","  return x\n","\n","inputs = Input(shape=(64, 64, 1))\n","\n","# Encoder\n","conv1 = my_conv(inputs,filters=8)\n","pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","conv2 = my_conv(pool1,filters=16)\n","pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","conv3 = my_conv(pool2,filters=32)\n","pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","conv4 = my_conv(pool3,filters=64)\n","\n","# Decoder\n","up7 = my_conv(conv4,filters=32)\n","up7 = UpSampling2D(size = (2,2))(up7)\n","merge7 = concatenate([conv3,up7], axis = 3)\n","up8 = my_conv(merge7,filters=16)\n","up8 = UpSampling2D(size = (2,2))(up8)\n","merge8 = concatenate([conv2,up8], axis = 3)\n","up9 = my_conv(merge8,filters=8)\n","up9 = UpSampling2D(size = (2,2))(up9)\n","merge9 = concatenate([conv1,up9], axis = 3)\n","\n","# Perform softmax on each pixel, so axis should be 3 because output has shape: batch_size x 64 x 64 x num_classes\n","conv11 = Conv2D(num_classes, 1, activation = softMaxAxis3)(merge9)\n","\n","model = Model(inputs, conv11)\n","model.summary()\n","model.compile(optimizer = keras.optimizers.RMSprop(learning_rate = 0.01), loss = 'mse')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pFtbTD9CC9v_"},"source":["###Questions 9.1\n","1. What does \"concatenate\" do?\n","2. Which pairs of layers of the encoder and decoder are being concatenated?\n","3. Why do you think U-Net concatenates outputs from layers of the encoder and layers of the decoder?\n","\n","Hint: Use K.int_shape(...) to get the shapes of the layers that are being concatenated"]},{"cell_type":"markdown","metadata":{"id":"By1_TT4pOrpu"},"source":["###Training"]},{"cell_type":"code","metadata":{"id":"iu1bZx4J65lM"},"source":["model.fit(x_train_seg, y_train_seg, epochs=20, batch_size=64, shuffle=True, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ECy9tSPLOuG2"},"source":["###Model evaluation"]},{"cell_type":"code","metadata":{"id":"fOz5f-5-7rn4"},"source":["# Pick 4 random examples\n","rand_ix = np.random.randint(0,5000,4)\n","out = model.predict(x_train_seg[rand_ix,:,:,:])\n","ref = y_train_seg[rand_ix,:,:,:].squeeze()\n","for k in range(4):\n","  plt.figure(figsize=(20,4))\n","  plt.subplot(2,11,1)\n","  plt.imshow(x_train_seg[rand_ix[k],:,:,:].squeeze())\n","  plt.title('Input image')\n","  ax.get_xaxis().set_visible(False)\n","  ax.get_yaxis().set_visible(False)\n","  for i in range(10):\n","    ax = plt.subplot(2,11,i+2)\n","    plt.imshow(out[k,:,:,i].squeeze(),vmin=0,vmax=1)\n","    plt.title('Predicted ' + str(i))\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","    ax = plt.subplot(2,11,11+i+2)\n","    plt.imshow(ref[k,:,:,i].squeeze(),vmin=0,vmax=1)\n","    plt.title('Ground truth ' + str(i))\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uEgIG9TB-eM-"},"source":["If you are not satisfied with the results, you could train the model for more epochs."]},{"cell_type":"markdown","metadata":{"id":"psBtdJUcFt_n"},"source":["###Task 9.1\n","Remove the concatenation layers (i.e., fill in the empty slots marked with ???). Then run the code block and train the model."]},{"cell_type":"code","metadata":{"id":"FE0al-4AYcc6"},"source":["inputs = Input(shape=(64, 64, 1))\n","\n","# Encoder\n","conv1 = my_conv(inputs,filters=8)\n","pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","conv2 = my_conv(pool1,filters=16)\n","pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","conv3 = my_conv(pool2,filters=32)\n","pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","conv4 = my_conv(pool3,filters=64)\n","\n","# Decoder\n","up7 = my_conv(conv4,filters=32)\n","up7 = UpSampling2D(size = (2,2))(up7)\n","#merge7 = concatenate([conv3,up7], axis = 3)\n","up8 = my_conv(???,filters=16)\n","up8 = UpSampling2D(size = (2,2))(up8)\n","#merge8 = concatenate([conv2,up8], axis = 3)\n","up9 = my_conv(???,filters=8)\n","up9 = UpSampling2D(size = (2,2))(up9)\n","#merge9 = concatenate([conv1,up9], axis = 3)\n","\n","# Perform softmax on each pixel, so axis should be 3 because output has shape: batch_size x 64 x 64 x num_classes\n","conv11 = Conv2D(num_classes, 1, activation = softMaxAxis3)(???)\n","\n","model = Model(inputs, conv11)\n","model.summary()\n","model.compile(optimizer = keras.optimizers.RMSprop(learning_rate = 0.01), loss = 'mse')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YCHTqAt7HLiH"},"source":["###Training\n","Train the model"]},{"cell_type":"code","metadata":{"id":"lVlwZA9SGPao"},"source":["model.fit(x_train_seg, y_train_seg, epochs=20, batch_size=64, shuffle=True, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"scV-vmHuHPLL"},"source":["###Model evaluation"]},{"cell_type":"code","metadata":{"id":"sRe5Y2gRGwGc"},"source":["# Pick 4 random examples\n","rand_ix = np.random.randint(0,5000,4)\n","out = model.predict(x_train_seg[rand_ix,:,:,:])\n","ref = y_train_seg[rand_ix,:,:,:].squeeze()\n","for k in range(4):\n","  plt.figure(figsize=(20,4))\n","  plt.subplot(2,11,1)\n","  plt.imshow(x_train_seg[rand_ix[k],:,:,:].squeeze())\n","  plt.title('Input image')\n","  ax.get_xaxis().set_visible(False)\n","  ax.get_yaxis().set_visible(False)\n","  for i in range(10):\n","    ax = plt.subplot(2,11,i+2)\n","    plt.imshow(out[k,:,:,i].squeeze(),vmin=0,vmax=1)\n","    plt.title('Predicted ' + str(i))\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","    ax = plt.subplot(2,11,11+i+2)\n","    plt.imshow(ref[k,:,:,i].squeeze(),vmin=0,vmax=1)\n","    plt.title('True ' + str(i))\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MsDvcgV9HS0j"},"source":["###Questions 9.2\n","1. The results look more blurry without the concatenation layers. Can you come up with an explanation why that is?"]},{"cell_type":"markdown","metadata":{"id":"LyRsmJa7zIk4"},"source":["##10 Ideas for further work:"]},{"cell_type":"markdown","metadata":{"id":"jJrlKE5CPWvz"},"source":["### Few-shot learning (or one-shot learning) with Siamese networks\n","\n","**Run this notebook:** https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/siamese_contrastive.ipynb\n","\n","**Your task:** Figure out what this is all about.\n","\n","The notebook doesn't explain it that well. I recommend [this blog post](https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d) instead.\n","\n","Questions that might help you:\n","- Why does this network architecture take two images as input?\n","- We have 10 classes in the MNIST dataset, but the Siamese network has only two output classes. Why? What do the two classes represent?\n","- What could possibly be the motivation for replacing our standard image classifier with a Siamese network?\n","\n","**Suggested task:** Train the Siamese network on a much smaller set of images (e.g., 100 images per class). Does it still work?\n","\n","Original source:\n","- https://keras.io/examples/vision/siamese_contrastive/\n","- https://keras.io/examples/vision/siamese_network/"]},{"cell_type":"markdown","metadata":{"id":"onlU4fruNQKQ"},"source":["**Explanation**\n","\n","A siamese takes not *one*, but two images as input (`A` and `B`). Both images are passed through the same encoder (`E`) to obtain feature vectors:\n","\n","```\n","a = E(A)\n","b = E(b)\n","```\n","\n","The decoder is then trained to predict whether the two input images are of the same class or not, based on the feature vectors a and b. Therefore the training set consists of an even number of pairs of images of \"same class\" and \"not same class\".\n","\n","Essentially what a siamese network learns is a distance metric\n","\n","```\n","D(a,b)\n","```\n","\n","such that `D(a,b)` is small if `A` and `B` are of the same class, and large if they are not of the same class.\n","\n","So this network is not learning to classify an image directly to any of the output classes. Rather, it is learning a similarity function, which takes two images as input and expresses how similar they are.\n","\n","This has two major advantages:\n","\n","a) You do not require too many instances of a class and only few are enough to build a good model. Why? Because you can generate many more training instances, because each training instance is a pair of images. From just 10 images, you can generate 10*10 = 100 unique image pairs.\n","\n","b) The biggest advantage is that, let’s say in case of face recognition, we have a new employee who has joined the organization. Now in order for the network to detect his face, we only require a single image of his face which will be stored in the database. Using this as the reference image, the network will calculate the similarity for any new instance presented to it. Thus we say that network predicts the score in one shot."]},{"cell_type":"markdown","metadata":{"id":"a_5WzyIk9LFB"},"source":["###Generative Adversarial Networks (GANs)\n","**Run this notebook:** https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/dcgan.ipynb#scrollTo=5x3q9_Oe5q0A\n","\n","**Task:** Figure out what this thing is doing. What could possibly be the application GANs?\n","\n","Read about DCGAN here: https://towardsdatascience.com/dcgans-deep-convolutional-generative-adversarial-networks-c7f392c2c8f8\n","\n","See other GAN examples here: https://keras.io/examples/generative/"]},{"cell_type":"markdown","metadata":{"id":"yVHtVbCfQ0_R"},"source":["**Explanation**\n","\n","DCGAN is short for Deep Convolutional Generative Adversarial Networks. A DCGAN is a generative model that learns to map random noise vectors into images. Unlike an autoencoder, which  encodes and decodes an image into itself, DCGAN learns to generate images that look real. This means that you must have a data set of real images to compare with.\n","\n","The network consists of two sub-networks that are trained in tandem:\n","\n","- The **Generator** takes a random noise vector and maps it into an image.\n","- The **Discriminator** takes an input image, which is either **\"real\"** (i.e., picked from the database of real images) or **\"fake\"** (i.e., generated by the Generator). It then learns to distingiush between real and fake images.\n","\n","The two networks are competing against each other, and at some point the Generator becomes so good at generating fakes, which look real, that the Discriminator can no longer distuingish fakes from reals.\n","\n","GANs are really hard to train and the above example is just a toy example."]},{"cell_type":"code","metadata":{"id":"QuCrfo9pTnx6"},"source":[],"execution_count":null,"outputs":[]}]}